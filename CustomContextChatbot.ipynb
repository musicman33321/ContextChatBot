{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "I chose the 2023 fashion trends dataset in order to provide more specific information surrounding trends of that time period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "## Data Wrangling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69b83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load CSV data\n",
    "df = pd.read_csv('data/2023_fashion_trends.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a595980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract text data columnn\n",
    "df_cleaned = df[\"Trends\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {},
   "source": [
    "## Custom Query Completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582f0656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def create_prompt(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Count the number of tokens in the prompt template and question\n",
    "    prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the question\n",
    "can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "\n",
    "    context = []\n",
    "    #for text in get_rows_sorted_by_relevance(question, df)[\"text\"].values:\n",
    "    for text in df:\n",
    "\n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        current_token_count += text_token_count\n",
    "\n",
    "        # Add the row of text to the list if we haven't exceeded the max\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return current_token_count, prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6e1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_count = 4097\n",
    "max_response_length = 1000\n",
    "question = \"What should I wear for a Y2K party?\"\n",
    "\n",
    "[num_tokens, prompt] = create_prompt(question, df_cleaned, max_token_count - max_response_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f2dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_base = \"https://openai.vocareum.com/v1\"\n",
    "openai.api_key = \"YOUR API KEY\"\n",
    "\n",
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "def answer_question(\n",
    "    question, df, max_prompt_tokens=1800, max_answer_tokens=150\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response, return the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "\n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "\n",
    "    [num_tokens, prompt] = create_prompt(question, df, max_prompt_tokens - max_answer_tokens)\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=COMPLETION_MODEL_NAME,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_answer_tokens\n",
    "        )\n",
    "        return num_tokens, response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c403f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "[num_tokens,response] = answer_question(\n",
    "    question, df_cleaned, max_prompt_tokens=4097, max_answer_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74280b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You could wear a sheer cellophane-like dress over a boldly hued maxi skirt, a slip satin long skirt with a white ribbed tank and pointed toe kitten heels, or a denim maxi skirt with a corresponding denim corset top and some chunky platforms. You could also consider incorporating pastels, denim-on-denim, or mesh pieces, as these trends were popular in the Y2K era. Ultimately, the fashion you choose to wear for a Y2K party should reflect your personal style and make you feel confident and stylish.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {},
   "source": [
    "## Custom Performance Demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11fdc0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4901c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "It's 2023, is a Canadian tuxedo in or out?\n",
      "\n",
      "Answer:\n",
      "In 2023, fashion trends are constantly changing, so it is difficult to predict if a Canadian tuxedo will be in or out. However, fashion experts believe that denim-on-denim looks will continue to be popular in the upcoming years, so it's possible that Canadian tuxedos will still be considered stylish. Ultimately, fashion is a personal choice, so wearing a Canadian tuxedo should be based on individual preference rather than trends.\n"
     ]
    }
   ],
   "source": [
    "#ChatGPT without provided fasion 2023 context:\n",
    "question1 = \"It's 2023, is a Canadian tuxedo in or out?\"\n",
    "\n",
    "max_response_length = 150\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=question1,\n",
    "    max_tokens=max_response_length\n",
    ")\n",
    "\n",
    "print(\"Question:\\n\" + question1 + \"\\n\")\n",
    "print(\"Answer:\\n\" + response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd7a093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "It's 2023, is a Canadian tuxedo in or out?\n",
      "\n",
      "Answer:\n",
      "In.\n"
     ]
    }
   ],
   "source": [
    "#ChatGPT with provided fasion 2023 context:\n",
    "_ , prompt_text= create_prompt(question1, df_cleaned, max_token_count- max_response_length)\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt_text,\n",
    "    max_tokens=max_response_length\n",
    ")\n",
    "\n",
    "print(\"Question:\\n\" + question1 + \"\\n\")\n",
    "print(\"Answer:\\n\" + response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e37c",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f646989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What size bags are fashionable in 2023?\n",
      "\n",
      "Answer:\n",
      "It is difficult to predict the exact size of bags that will be popular in 2023. Fashion trends constantly change and evolve, and what is considered fashionable can vary greatly from year to year. However, some predictions for popular bag sizes in 2023 include:\n",
      "\n",
      "1. Oversized bags: Large, roomy bags have been popular for a few years now and are likely to continue being trendy in 2023. These bags are not only fashionable, but also practical, as they allow you to carry all your essentials without having to sacrifice style.\n",
      "\n",
      "2. Mini bags: On the other end of the spectrum, mini bags are also expected to remain popular in 2023. These tiny bags are perfect for occasions where you only need to carry\n"
     ]
    }
   ],
   "source": [
    "#ChatGPT without provided fasion 2023 context:\n",
    "question2 = \"What size bags are fashionable in 2023?\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=question2,\n",
    "    max_tokens=max_response_length\n",
    ")\n",
    "\n",
    "print(\"Question:\\n\" + question2 + \"\\n\")\n",
    "print(\"Answer:\\n\" + response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11c07a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What size bags are fashionable in 2023?\n",
      "\n",
      "Answer:\n",
      "Oversized bags are fashionable in 2023, as seen on the runway and predicted by fashion experts and stylists.\n"
     ]
    }
   ],
   "source": [
    "#ChatGPT with provided fasion 2023 context:\n",
    "_ , prompt_text= create_prompt(question2, df_cleaned, max_token_count- max_response_length)\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt_text,\n",
    "    max_tokens=max_response_length\n",
    ")\n",
    "\n",
    "print(\"Question:\\n\" + question2 + \"\\n\")\n",
    "print(\"Answer:\\n\" + response[\"choices\"][0][\"text\"].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
